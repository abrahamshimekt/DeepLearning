{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neccessary Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "ZT76Mn5mKIMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TohkSEjQmdzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "EKK-Ho8KKcVA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ],
      "metadata": {
        "id": "1AHAk0smLAe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_ReLU:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Convert inputs to a PyTorch tensor if they're not already\n",
        "        inputs = torch.tensor(inputs)\n",
        "\n",
        "        # Apply ReLU activation\n",
        "        self.output = torch.max(torch.tensor(0), inputs)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Hg0DTaSK8Vd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Softmax:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Convert inputs to a PyTorch tensor if they're not already\n",
        "        inputs = torch.tensor(inputs)\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = torch.exp(inputs - torch.max(inputs, dim=-1, keepdim=True).values)\n",
        "\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / torch.sum(exp_values, dim=-1, keepdim=True)\n",
        "        self.output = probabilities\n",
        "        return self.output\n",
        "\n"
      ],
      "metadata": {
        "id": "L453kdmX9lk6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Sigmoid:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Convert inputs to a PyTorch tensor if they're not already\n",
        "        inputs = torch.tensor(inputs)\n",
        "\n",
        "        # Apply the sigmoid activation function\n",
        "        self.output = 1 / (1 + torch.exp(-inputs))\n",
        "        return self.output\n"
      ],
      "metadata": {
        "id": "bDMzbrAH9qSt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model"
      ],
      "metadata": {
        "id": "5QlBHU5TLLM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, num_features, num_neurons):\n",
        "        # Initialize weights and biases for given layer\n",
        "        self.weights = torch.randn(num_features, num_neurons)\n",
        "        self.bias = torch.zeros(1, num_neurons)\n",
        "\n",
        "        # Dictionary to store activation functions\n",
        "        self.activations = {\n",
        "            'softmax': Activation_Softmax(),\n",
        "            'relu': Activation_ReLU(),\n",
        "            'sigmoid': Activation_Sigmoid()\n",
        "        }\n",
        "\n",
        "    def forward(self, x, activation_function_name):\n",
        "        if activation_function_name not in self.activations:\n",
        "            raise ValueError(\"Activation function not supported\")\n",
        "\n",
        "        weight_output = torch.matmul(x, self.weights) + self.bias\n",
        "        activation = self.activations[activation_function_name]\n",
        "        output = activation.forward(weight_output)\n",
        "\n",
        "        self.output = output\n",
        "        return self.output\n"
      ],
      "metadata": {
        "id": "Rap85cw8J_Ic"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring Loss"
      ],
      "metadata": {
        "id": "VqcHAxjee22V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_CategoricalCrossentropy:\n",
        "       def forward(self, y_pred, y_true):\n",
        "\n",
        "          # Clip values to avoid log(0) or log(1)\\n\",\n",
        "\n",
        "          y_pred_clipped = torch.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "          log_likelihoods = -torch.sum(y_true * torch.log(y_pred_clipped))\n",
        "\n",
        "          return log_likelihoods"
      ],
      "metadata": {
        "id": "GTwExQm-KQvF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model using ReLU for hidden layers"
      ],
      "metadata": {
        "id": "ngFzPUt4LddD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network layers\n",
        "layer1 = NeuralNetwork(4, 18)\n",
        "layer2 = NeuralNetwork(18, 18)\n",
        "layer3 = NeuralNetwork(18, 18)\n",
        "output_layer = NeuralNetwork(18, 3)\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(input_data):\n",
        "    layer1_out = layer1.forward(input_data, \"relu\")\n",
        "    layer2_out = layer2.forward(layer1_out, \"relu\")\n",
        "    layer3_out = layer3.forward(layer2_out, \"relu\")\n",
        "    final_output = output_layer.forward(layer3_out, \"softmax\")\n",
        "    return final_output\n",
        "\n",
        "torch.manual_seed(42)\n",
        "input_data = torch.rand((1, 4))\n",
        "\n",
        "# Perform forward pass\n",
        "output = forward_pass(input_data)\n",
        "\n",
        "# Compute loss and accuracy\n",
        "target = torch.tensor([1])\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "loss = loss_function.forward(output, target)\n",
        "accuracy = target == torch.argmax(output, axis=1)\n",
        "\n",
        "# Print results\n",
        "print(\"Final output:\", output)\n",
        "print(\"Categorical Cross-Entropy Loss:\", loss.item())\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "NvpQISwWLiZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fa16a1-1598-4314-e67d-8b7c0acb3164"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output: tensor([[4.5772e-12, 1.0000e+00, 5.5234e-41]])\n",
            "Categorical Cross-Entropy Loss: 32.23619079589844\n",
            "Accuracy: tensor([True])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-ca3b9c80b5a3>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(inputs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model using Sigmoid for hidden layers"
      ],
      "metadata": {
        "id": "4ss9uB5PLxiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network layers\n",
        "layer1 = NeuralNetwork(4, 18)\n",
        "layer2 = NeuralNetwork(18, 18)\n",
        "layer3 = NeuralNetwork(18, 18)\n",
        "output_layer = NeuralNetwork(18, 3)\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(input_data):\n",
        "    layer1_out = layer1.forward(input_data, \"sigmoid\")\n",
        "    layer2_out = layer2.forward(layer1_out, \"sigmoid\")\n",
        "    layer3_out = layer3.forward(layer2_out, \"sigmoid\")\n",
        "    final_output = output_layer.forward(layer3_out, \"softmax\")\n",
        "    return final_output\n",
        "\n",
        "torch.manual_seed(42)\n",
        "input_data = torch.rand((1, 4))\n",
        "\n",
        "# Perform forward pass\n",
        "output = forward_pass(input_data)\n",
        "\n",
        "# Compute loss and accuracy\n",
        "target = torch.tensor([1])\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "loss = loss_function.forward(output, target)\n",
        "accuracy = target == torch.argmax(output, axis=1)\n",
        "\n",
        "# Print results\n",
        "print(\"Final output:\", output)\n",
        "print(\"Categorical Cross-Entropy Loss:\", loss.item())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z9PmPJOLr8X",
        "outputId": "12fe3ee8-2ac5-406f-a3be-cc7e372eb931"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output: tensor([[1.0227e-01, 8.9742e-01, 3.0958e-04]])\n",
            "Categorical Cross-Entropy Loss: 10.468658447265625\n",
            "Accuracy: tensor([True])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-2c20c1605d3b>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(inputs)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}