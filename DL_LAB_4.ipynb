{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.478189Z","iopub.execute_input":"2023-11-30T19:03:50.478606Z","iopub.status.idle":"2023-11-30T19:03:50.483313Z","shell.execute_reply.started":"2023-11-30T19:03:50.478563Z","shell.execute_reply":"2023-11-30T19:03:50.482113Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class Activation_Sigmoid:\n    def forward(self, x):\n        self.output = 1 / (1 + torch.exp(-x))\n    \n    def backward(self):\n        return self.output * (1 - self.output)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.492235Z","iopub.execute_input":"2023-11-30T19:03:50.493263Z","iopub.status.idle":"2023-11-30T19:03:50.498312Z","shell.execute_reply.started":"2023-11-30T19:03:50.493228Z","shell.execute_reply":"2023-11-30T19:03:50.497513Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class Activation_Linear:\n    def forward(self, x):\n        self.output = x\n    \n    def backward(self):\n        return torch.ones_like(self.output)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.499838Z","iopub.execute_input":"2023-11-30T19:03:50.500778Z","iopub.status.idle":"2023-11-30T19:03:50.511831Z","shell.execute_reply.started":"2023-11-30T19:03:50.500749Z","shell.execute_reply":"2023-11-30T19:03:50.510649Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class DenseLayer:\n    def __init__(self, num_inputs, num_neurons):\n        self.weights = 0.001 * torch.rand(num_inputs, num_neurons, requires_grad=True)\n        self.biases = torch.rand((1, num_neurons), requires_grad=True)\n\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = torch.matmul(inputs, self.weights) + self.biases\n\n    def backward(self, grad):\n        self.weights.grad = torch.matmul(self.inputs.T, grad)\n        self.biases.grad = torch.sum(grad, dim=0, keepdim=True)\n        return torch.matmul(grad, self.weights.T)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.513398Z","iopub.execute_input":"2023-11-30T19:03:50.514290Z","iopub.status.idle":"2023-11-30T19:03:50.524503Z","shell.execute_reply.started":"2023-11-30T19:03:50.514217Z","shell.execute_reply":"2023-11-30T19:03:50.523376Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"hidden_layer = DenseLayer(2, 4)\nactivation1 = Activation_Sigmoid()\noutput_layer = DenseLayer(4, 2)\nactivation2 = Activation_Linear()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.526767Z","iopub.execute_input":"2023-11-30T19:03:50.527308Z","iopub.status.idle":"2023-11-30T19:03:50.535075Z","shell.execute_reply.started":"2023-11-30T19:03:50.527268Z","shell.execute_reply":"2023-11-30T19:03:50.533918Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def forward_pass(X):\n    hidden_layer.forward(X)\n    activation1.forward(hidden_layer.output)\n    output_layer.forward(activation1.output)\n    activation2.forward(output_layer.output)\n    return activation2.output","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.536387Z","iopub.execute_input":"2023-11-30T19:03:50.536890Z","iopub.status.idle":"2023-11-30T19:03:50.549814Z","shell.execute_reply.started":"2023-11-30T19:03:50.536858Z","shell.execute_reply":"2023-11-30T19:03:50.548867Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def back_prop(y_pred):\n    # Calculate the loss\n    loss = 0.5 * torch.mean((y - y_pred) ** 2)\n    \n    # Compute the gradient of the loss\n    grad = y_pred - y\n    \n    # Backpropagate through the layers starting from the output layer\n    # Calculate gradients with respect to the layers and activations\n    \n    activation2_grad = activation2.backward() * grad\n    output_layer_grad = output_layer.backward(activation2_grad)\n    activation1_grad = activation1.backward() * output_layer_grad\n    hidden_layer.backward(activation1_grad)\n    \n    # Update weights and biases using the gradients with a learning rate of 0.01\n    # We can iterate over the layers instead of hardcoding everything\n    with torch.no_grad():\n        for layer in [hidden_layer, output_layer]:\n            layer.weights -= 0.01 * layer.weights.grad\n            layer.biases -= 0.01 * layer.biases.grad\n            \n            # Reset gradients to zero after updating weights and biases\n            layer.weights.grad.zero_()\n            layer.biases.grad.zero_()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.550950Z","iopub.execute_input":"2023-11-30T19:03:50.552007Z","iopub.status.idle":"2023-11-30T19:03:50.562270Z","shell.execute_reply.started":"2023-11-30T19:03:50.551945Z","shell.execute_reply":"2023-11-30T19:03:50.561343Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"\nloss_threshold = 0.0001\ny_pred = forward_pass(X)\nerr = torch.mean(0.5 * (y - y_pred) ** 2)\nprint(\"Initial loss:\", err.item())\nprint(\"Initial prediction:\", y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.563555Z","iopub.execute_input":"2023-11-30T19:03:50.564061Z","iopub.status.idle":"2023-11-30T19:03:50.576336Z","shell.execute_reply.started":"2023-11-30T19:03:50.564033Z","shell.execute_reply":"2023-11-30T19:03:50.575168Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Initial loss: 0.21378612518310547\nInitial prediction: tensor([[0.9127, 0.6170]], grad_fn=<AddBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"while err > loss_threshold:\n    back_prop(y_pred)\n    y_pred = forward_pass(X)\n    err = torch.mean(0.5 * (y - y_pred) ** 2)\n    print(\"Current loss:\", err.item())\n\nprint(\"Final loss:\", err.item())\nprint(\"Final prediction:\", y_pred)\nprint(\"Target value:\", y)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:03:50.577453Z","iopub.execute_input":"2023-11-30T19:03:50.578763Z","iopub.status.idle":"2023-11-30T19:03:50.638704Z","shell.execute_reply.started":"2023-11-30T19:03:50.578700Z","shell.execute_reply":"2023-11-30T19:03:50.637698Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Current loss: 0.20310473442077637\nCurrent loss: 0.19295696914196014\nCurrent loss: 0.18331599235534668\nCurrent loss: 0.17415645718574524\nCurrent loss: 0.1654542237520218\nCurrent loss: 0.157186359167099\nCurrent loss: 0.1493312120437622\nCurrent loss: 0.14186805486679077\nCurrent loss: 0.13477736711502075\nCurrent loss: 0.12804046273231506\nCurrent loss: 0.12163974344730377\nCurrent loss: 0.11555841565132141\nCurrent loss: 0.10978050529956818\nCurrent loss: 0.1042909026145935\nCurrent loss: 0.09907524287700653\nCurrent loss: 0.0941198319196701\nCurrent loss: 0.08941169828176498\nCurrent loss: 0.08493853360414505\nCurrent loss: 0.08068863302469254\nCurrent loss: 0.07665086537599564\nCurrent loss: 0.07281464338302612\nCurrent loss: 0.06916993856430054\nCurrent loss: 0.06570722162723541\nCurrent loss: 0.062417395412921906\nCurrent loss: 0.05929188430309296\nCurrent loss: 0.05632247030735016\nCurrent loss: 0.05350140109658241\nCurrent loss: 0.05082125961780548\nCurrent loss: 0.04827503114938736\nCurrent loss: 0.045856039971113205\nCurrent loss: 0.043557967990636826\nCurrent loss: 0.04137475788593292\nCurrent loss: 0.03930069878697395\nCurrent loss: 0.03733035922050476\nCurrent loss: 0.03545854240655899\nCurrent loss: 0.033680349588394165\nCurrent loss: 0.03199111297726631\nCurrent loss: 0.030386392027139664\nCurrent loss: 0.02886197715997696\nCurrent loss: 0.02741384692490101\nCurrent loss: 0.026038208976387978\nCurrent loss: 0.02473144233226776\nCurrent loss: 0.023490099236369133\nCurrent loss: 0.022310923784971237\nCurrent loss: 0.02119080349802971\nCurrent loss: 0.020126793533563614\nCurrent loss: 0.019116101786494255\nCurrent loss: 0.01815604418516159\nCurrent loss: 0.01724410057067871\nCurrent loss: 0.016377871856093407\nCurrent loss: 0.015555066987872124\nCurrent loss: 0.01477351225912571\nCurrent loss: 0.014031151309609413\nCurrent loss: 0.013326023705303669\nCurrent loss: 0.012656256556510925\nCurrent loss: 0.012020095251500607\nCurrent loss: 0.01141585037112236\nCurrent loss: 0.010841920971870422\nCurrent loss: 0.010296791791915894\nCurrent loss: 0.009779022075235844\nCurrent loss: 0.009287250228226185\nCurrent loss: 0.008820165880024433\nCurrent loss: 0.0083765322342515\nCurrent loss: 0.007955175824463367\nCurrent loss: 0.007554981391876936\nCurrent loss: 0.007174883969128132\nCurrent loss: 0.006813880987465382\nCurrent loss: 0.0064710164442658424\nCurrent loss: 0.006145380437374115\nCurrent loss: 0.005836104042828083\nCurrent loss: 0.005542372819036245\nCurrent loss: 0.00526340352371335\nCurrent loss: 0.004998456221073866\nCurrent loss: 0.004746824968606234\nCurrent loss: 0.004507849924266338\nCurrent loss: 0.004280891269445419\nCurrent loss: 0.004065339919179678\nCurrent loss: 0.003860635217279196\nCurrent loss: 0.0036662218626588583\nCurrent loss: 0.0034815899562090635\nCurrent loss: 0.003306242171674967\nCurrent loss: 0.0031397161073982716\nCurrent loss: 0.0029815707821398973\nCurrent loss: 0.002831381745636463\nCurrent loss: 0.0026887496933341026\nCurrent loss: 0.002553296973928809\nCurrent loss: 0.002424658741801977\nCurrent loss: 0.0023024973925203085\nCurrent loss: 0.0021864823065698147\nCurrent loss: 0.002076308475807309\nCurrent loss: 0.001971680438145995\nCurrent loss: 0.0018723204266279936\nCurrent loss: 0.0017779649933800101\nCurrent loss: 0.0016883586067706347\nCurrent loss: 0.001603267272002995\nCurrent loss: 0.0015224587405100465\nCurrent loss: 0.0014457202050834894\nCurrent loss: 0.001372843747958541\nCurrent loss: 0.0013036407763138413\nCurrent loss: 0.0012379232794046402\nCurrent loss: 0.0011755162850022316\nCurrent loss: 0.0011162534356117249\nCurrent loss: 0.0010599761735647917\nCurrent loss: 0.0010065337410196662\nCurrent loss: 0.000955784460529685\nCurrent loss: 0.0009075921261683106\nCurrent loss: 0.0008618292049504817\nCurrent loss: 0.0008183708996511996\nCurrent loss: 0.0007771041127853096\nCurrent loss: 0.0007379176677204669\nCurrent loss: 0.0007007035310380161\nCurrent loss: 0.0006653674645349383\nCurrent loss: 0.0006318119121715426\nCurrent loss: 0.000599948107264936\nCurrent loss: 0.0005696893786080182\nCurrent loss: 0.0005409552250057459\nCurrent loss: 0.0005136707914061844\nCurrent loss: 0.00048776145558804274\nCurrent loss: 0.00046315803774632514\nCurrent loss: 0.0004397957818582654\nCurrent loss: 0.0004176115617156029\nCurrent loss: 0.00039654504507780075\nCurrent loss: 0.00037654105108231306\nCurrent loss: 0.00035754675627686083\nCurrent loss: 0.00033950962824746966\nCurrent loss: 0.0003223824314773083\nCurrent loss: 0.00030611766851507127\nCurrent loss: 0.0002906742156483233\nCurrent loss: 0.0002760096685960889\nCurrent loss: 0.0002620844170451164\nCurrent loss: 0.0002488612080924213\nCurrent loss: 0.0002363058301853016\nCurrent loss: 0.00022438321320805699\nCurrent loss: 0.00021306125563569367\nCurrent loss: 0.0002023117122007534\nCurrent loss: 0.00019210344180464745\nCurrent loss: 0.0001824100618250668\nCurrent loss: 0.0001732054224703461\nCurrent loss: 0.00016446554218418896\nCurrent loss: 0.00015616639575455338\nCurrent loss: 0.00014828650455456227\nCurrent loss: 0.00014080334221944213\nCurrent loss: 0.00013369777298066765\nCurrent loss: 0.0001269508502446115\nCurrent loss: 0.00012054429680574685\nCurrent loss: 0.0001144608249887824\nCurrent loss: 0.000108685067971237\nCurrent loss: 0.00010319967259420082\nCurrent loss: 9.799169492907822e-05\nFinal loss: 9.799169492907822e-05\nFinal prediction: tensor([[0.0685, 0.9429]], grad_fn=<AddBackward0>)\nTarget value: tensor([[0.0500, 0.9500]])\n","output_type":"stream"}]}]}